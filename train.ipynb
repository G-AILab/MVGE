{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import train_test_split_edges,negative_sampling\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from model_MVGE_three_loss_gcn_no_concat_para import MVGE1,MVGE2\n",
    "from input_data import *\n",
    "from dataset import CustomDataset\n",
    "import pickle\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "# torch.cuda.set_device(0)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cora Have 2708 Nodes, 10556 Edges, 1433 Attribute, 7 Classes\n",
      "-----------start aggregate neighbor-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c5b19154504a62a33b7848f397d790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2708.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------process data completed-----------\n",
      "LogisticRegression(n_jobs=10)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08e3d144a874f2fbfa722ee40909683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             ratio 0.1  ratio 0.3  ratio 0.5  ratio 0.7  ratio 0.9\n",
      "train-micro   0.844444   0.863311   0.867066   0.864380   0.867461\n",
      "micro-std     0.050239   0.033287   0.043725   0.038287   0.030641\n",
      "train-macro   0.822508   0.860374   0.850393   0.851723   0.855697\n",
      "macro-std     0.072973   0.040244   0.055275   0.040346   0.036217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-4-f36bbdd0d60d>\", line 38, in <module>\n",
      "    loss = model.loss(data.x,data.x_neighbor, data.edge_index, data.edge_index,a,b)\n",
      "  File \"/notebooks/MVGE-main/model_MVGE_three_loss_gcn_no_concat_para.py\", line 180, in loss\n",
      "    neg_edge_index = negative_sampling(all_edge_index_tmp, z_self.size(0), pos_edge_index.size(1))\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch_geometric/utils/negative_sampling.py\", line 76, in negative_sampling\n",
      "    perm = sample(size, int(alpha * num_neg_samples))\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch_geometric/utils/negative_sampling.py\", line 12, in sample\n",
      "    return torch.tensor(random.sample(range(high), size), device=device)\n",
      "  File \"/usr/lib/python3.6/random.py\", line 340, in sample\n",
      "    result[i] = population[j]\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/usr/lib/python3.6/posixpath.py\", line 385, in abspath\n",
      "    return normpath(path)\n",
      "  File \"/usr/lib/python3.6/posixpath.py\", line 366, in normpath\n",
      "    new_comps.append(comp)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "dataset_name = 'cora'\n",
    "link_pred = True #whether link prediction task\n",
    "multi_view = True\n",
    "\n",
    "weight = {'cora':[0.1,0.7],'citeseer':[0.1,0.1],'pubmed':[0.5,0.4],'cora_full':[0.1,0.1],'chamelemon':[0.3,0],'cornell':[0.7,1.0],'texas':[0.7,1.0],'wisconsin':[0.9,1.0]}\n",
    "a = weight.get(dataset_name)[0]\n",
    "b = weight.get(dataset_name)[1]\n",
    "\n",
    "if 'syn' in dataset_name:\n",
    "    assor = \"h0.10-r1\" #the global homohpiyl of synthetic dataset\n",
    "    dataset = CustomDataset(root=\"./input/{}\".format(dataset_name), name=assor, setting=\"gcn\", seed=15)\n",
    "    G=nx.DiGraph(dataset.adj)\n",
    "    G_label = dataset.labels\n",
    "    G_attr = pd.DataFrame(dataset.features.toarray())\n",
    "    G_attr['nodes'] = G_attr.index\n",
    "else:\n",
    "    iG,G,G_label,G_attr = read_data(dataset_name)\n",
    "    \n",
    "\n",
    "data = process_data(G,G_attr,link_pred,multi_view)\n",
    "data = data.to(device)\n",
    "\n",
    "#training ten times\n",
    "t = np.zeros((5,4))\n",
    "for i in range(10):\n",
    "    out_channels=128\n",
    "    lr = 0.001\n",
    "    epoch = 200\n",
    "\n",
    "    model = MVGE1(data.x.shape[1],data.x_neighbor.shape[1],out_channels).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    os.makedirs(\"datasets\", exist_ok=True)\n",
    "\n",
    "    #start training\n",
    "    for epoch in range(epoch):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(data.x,data.x_neighbor, data.edge_index, data.edge_index,a,b)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #start evaluating\n",
    "    z,z1,z2 = model.embed(data.x,data.x_neighbor,data.edge_index)\n",
    "    node_embedding = z.cpu().detach().numpy()\n",
    "    node_embedding = pd.DataFrame(node_embedding)\n",
    "\n",
    "\n",
    "    k = get_all_cv_score(node_embedding, G, G_label, [LogisticRegression(n_jobs=10)])\n",
    "    tr = pd.DataFrame(k).T\n",
    "    ratios = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    tr.columns = ['ratio {}'.format(j) for j in ratios]\n",
    "    tr.index = ['train-micro', 'micro-std','train-macro','macro-std']\n",
    "    print(tr)\n",
    "    t = pd.DataFrame(k) + pd.DataFrame(t)\n",
    "    del model\n",
    "\n",
    "for tt in [t]:\n",
    "    tt = tt/10\n",
    "    k=[]\n",
    "    ratios = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    for i in range(len(ratios)): \n",
    "        micro = \"%0.4f±%0.4f\" % (tt.iloc[i][0],tt.iloc[i][1])\n",
    "        macro = \"%0.4f±%0.4f\" % (tt.iloc[i][2],tt.iloc[i][3])\n",
    "        k.append([micro, macro])\n",
    "\n",
    "    tr = pd.DataFrame(k).T\n",
    "    tr.columns = ['ratio {}'.format(j) for j in ratios]\n",
    "    tr.index = ['train-micro', 'train-macro']\n",
    "    display(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### link prediction\n",
    "\n",
    "link prediction task do not need to set the loss weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight = [0,0.2,0.4,0.6,0.8,1]\n",
    "dataset_name = 'cora'\n",
    "link_pred = True #whether link prediction task\n",
    "multi_view = True\n",
    "\n",
    "\n",
    "if 'syn' in dataset_name:\n",
    "    assor = \"h0.10-r1\" #the global homohpiyl of synthetic dataset\n",
    "    dataset = CustomDataset(root=\"./input/datasets/{}\".format(dataset_name), name=assor, setting=\"gcn\", seed=15)\n",
    "    G=nx.DiGraph(dataset.adj)\n",
    "    G_label = dataset.labels\n",
    "    G_attr = pd.DataFrame(dataset.features.toarray())\n",
    "    G_attr['nodes'] = G_attr.index\n",
    "else:\n",
    "    iG,G,G_label,G_attr = read_data(dataset_name)\n",
    "\n",
    "data = process_data(G,G_attr,link_pred,multi_view)\n",
    "data = data.to(device)\n",
    "from torch_geometric.utils import negative_sampling, remove_self_loops, add_self_loops\n",
    "all_edge_index_tmp, _ = remove_self_loops(data.edge_index)\n",
    "all_edge_index_tmp, _ = add_self_loops(all_edge_index_tmp)\n",
    "neg_edge_index = negative_sampling(all_edge_index_tmp, G_attr.shape[0], data.train_pos_edge_index.size(1))\n",
    "\n",
    "\n",
    "out_channels=128\n",
    "lr = 0.001\n",
    "epoch = 200\n",
    "aucs, aps = [], []\n",
    "\n",
    "#training ten times\n",
    "for i in range(10):\n",
    "    model = MVGE2(data.x.shape[1],data.x_neighbor.shape[1],out_channels).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    os.makedirs(\"datasets\", exist_ok=True)\n",
    "    best_auc = 0\n",
    "    best_ap = 0\n",
    "\n",
    "    #start training\n",
    "    for epoch in range(epoch):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(data.x,data.x_neighbor, data.train_pos_edge_index, data.edge_index,a,b)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #start evaluating\n",
    "    z,z1,z2 = model.embed(data.x,data.x_neighbor,data.edge_index)\n",
    "    del model\n",
    "    train = torch.cat([data.train_pos_edge_index,neg_edge_index],axis=1)\n",
    "    test = torch.cat([data.test_pos_edge_index,data.test_neg_edge_index],axis=1)\n",
    "    nodes_first = z[train[0],:]\n",
    "    nodes_second = z[train[1],:]\n",
    "    pred_train = nodes_first*nodes_second\n",
    "    pred_train = np.array(pred_train.cpu().detach().numpy().sum(axis=1)).reshape(-1,1)\n",
    "    label1 = np.ones([data.train_pos_edge_index.shape[1],])\n",
    "    label0 = np.zeros([neg_edge_index.shape[1],])\n",
    "    label_train = np.concatenate((label1,label0))\n",
    "\n",
    "    nodes_first = z[test[0],:]\n",
    "    nodes_second = z[test[1],:]\n",
    "    pred_test = nodes_first*nodes_second\n",
    "    pred_test = np.array(pred_test.cpu().detach().numpy().sum(axis=1)).reshape(-1,1)\n",
    "    label1 = np.ones([data.test_pos_edge_index.shape[1],])\n",
    "    label0 = np.zeros([data.test_neg_edge_index.shape[1],])\n",
    "    label_test = np.concatenate((label1,label0))\n",
    "\n",
    "    clf = LogisticRegressionCV(Cs=10,max_iter=100,n_jobs=10,verbose=1,scoring='roc_auc') \n",
    "    clf.fit(pred_train,label_train)\n",
    "    roc_auc = roc_auc_score(label_test,clf.predict_proba(pred_test)[:,1])\n",
    "    ap = average_precision_score(label_test,clf.predict_proba(pred_test)[:,1])\n",
    "    print('roc_auc:{:.4f}, ap:{:.4f}'.format(roc_auc,ap))\n",
    "\n",
    "    aucs.append(roc_auc)\n",
    "    aps.append(ap)\n",
    "\n",
    "print('End of Tainning {}! weight of loss:'.format(i),a,b)\n",
    "print('AUC ROC :{:.4f}±{:.4f}'.format(np.mean(aucs),np.std(aucs)))\n",
    "print('AP: {:.4f}±{:.4f}'.format(np.mean(aps),np.std(aps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise node classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_wise_score(node_embedding,name,test_size):\n",
    "    iG,G,G_label,G_attr = read_data(name)\n",
    "    mask_link_positive = pd.read_pickle(\"./input/pairwise_node/{}_mask_link_positive.pickle\".format(name))\n",
    "    mask_link_negtive = pd.read_pickle(\"./input/pairwise_node/{}_mask_link_negtive.pickle\".format(name))\n",
    "\n",
    "    num = len(G.edges())\n",
    "    mask_link_positive = mask_link_positive.T.sample(n=num).reset_index(drop=True)\n",
    "    mask_link_negtive = mask_link_negtive.sample(n=num).reset_index(drop=True)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    mask_link_positive_train, mask_link_positive_test = train_test_split(mask_link_positive,test_size=test_size)\n",
    "    mask_link_negtive_train, mask_link_negtive_test = train_test_split(mask_link_negtive,test_size=test_size)\n",
    "    train = pd.concat([mask_link_positive_train,mask_link_negtive_train],axis=0)\n",
    "    test = pd.concat([mask_link_positive_test,mask_link_negtive_test],axis=0)\n",
    "\n",
    "    nodes_first = node_embedding.loc[train[0]].reset_index(drop=True)\n",
    "    nodes_second = node_embedding.loc[train[1]].reset_index(drop=True)\n",
    "    train_pred = pd.DataFrame.mul(nodes_first,nodes_second)\n",
    "    train_label = [1 for i in range(train.shape[0]//2)] + [0 for i in range(train.shape[0]//2)]\n",
    "\n",
    "    nodes_first = node_embedding.loc[test[0]].reset_index(drop=True)\n",
    "    nodes_second = node_embedding.loc[test[1]].reset_index(drop=True)\n",
    "    test_pred = pd.DataFrame.mul(nodes_first,nodes_second)\n",
    "    test_label = [1 for i in range(test.shape[0]//2)] + [0 for i in range(test.shape[0]//2)]\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegressionCV\n",
    "    clf = LogisticRegressionCV(cv=5,Cs=10,max_iter=100,n_jobs=20,verbose=1,scoring='roc_auc')\n",
    "\n",
    "    clf.fit(train_pred,train_label)\n",
    "    auc, ap = roc_auc_score(test_label,clf.predict_proba(test_pred)[:,1]),average_precision_score(test_label,clf.predict_proba(test_pred)[:,1])\n",
    "    return auc,ap\n",
    "\n",
    "def get_pair_wise(name,test_size):\n",
    "    iG,G,G_label,G_attr = read_data(name)\n",
    "    mask_link_positive = pd.read_pickle(\"./input/pairwise_node/{}_mask_link_positive.pickle\".format(name))\n",
    "    mask_link_negtive = pd.read_pickle(\"./input/pairwise_node/{}_mask_link_negtive.pickle\".format(name))\n",
    "    \n",
    "    num = len(G.edges())\n",
    "    mask_link_positive = mask_link_positive.T.sample(n=num).reset_index(drop=True)\n",
    "    mask_link_negtive = mask_link_negtive.sample(n=num).reset_index(drop=True)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    mask_link_positive_train, mask_link_positive_test = train_test_split(mask_link_positive,test_size=test_size)\n",
    "    mask_link_negtive_train, mask_link_negtive_test = train_test_split(mask_link_negtive,test_size=test_size)\n",
    "    train = pd.concat([mask_link_positive_train,mask_link_negtive_train],axis=0)\n",
    "    test = pd.concat([mask_link_positive_test,mask_link_negtive_test],axis=0)\n",
    "    \n",
    "    train_label = [1 for i in range(train.shape[0]//2)] + [0 for i in range(train.shape[0]//2)]\n",
    "    test_label = [1 for i in range(test.shape[0]//2)] + [0 for i in range(test.shape[0]//2)]\n",
    "    return train,test,train_label,test_label\n",
    "\n",
    "def get_score(node_embedding,train,test,train_label,test_label):\n",
    "    nodes_first = node_embedding.loc[train[0]].reset_index(drop=True)\n",
    "    nodes_second = node_embedding.loc[train[1]].reset_index(drop=True)\n",
    "    train_pred = pd.DataFrame.mul(nodes_first,nodes_second)\n",
    "\n",
    "    nodes_first = node_embedding.loc[test[0]].reset_index(drop=True)\n",
    "    nodes_second = node_embedding.loc[test[1]].reset_index(drop=True)\n",
    "    test_pred = pd.DataFrame.mul(nodes_first,nodes_second)\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegressionCV\n",
    "    clf = LogisticRegressionCV(cv=5,Cs=10,max_iter=100,n_jobs=20,verbose=1,scoring='roc_auc')\n",
    "\n",
    "    clf.fit(train_pred,train_label)\n",
    "    auc, ap = roc_auc_score(test_label,clf.predict_proba(test_pred)[:,1]),average_precision_score(test_label,clf.predict_proba(test_pred)[:,1])\n",
    "    return auc,ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cora Have 2708 Nodes, 10556 Edges, 1433 Attribute, 7 Classes\n",
      "-----------start aggregate neighbor-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e04767e760461cb1965b5f366b88d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2708.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------process data completed-----------\n",
      "cora Have 2708 Nodes, 10556 Edges, 1433 Attribute, 7 Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   2 out of   5 | elapsed:    4.1s remaining:    6.1s\n",
      "[Parallel(n_jobs=20)]: Done   5 out of   5 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   2 out of   5 | elapsed:    3.1s remaining:    4.6s\n",
      "[Parallel(n_jobs=20)]: Done   5 out of   5 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   2 out of   5 | elapsed:    2.6s remaining:    3.9s\n",
      "[Parallel(n_jobs=20)]: Done   5 out of   5 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   2 out of   5 | elapsed:    3.3s remaining:    4.9s\n",
      "[Parallel(n_jobs=20)]: Done   5 out of   5 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s\n",
      "[Parallel(n_jobs=20)]: Done   5 out of   5 | elapsed:    2.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Tainning of 10 times!\n",
      "AUC ROC :0.8333±0.0097\n",
      "AP: 0.8345±0.0075\n"
     ]
    }
   ],
   "source": [
    "weight = [0,0.2,0.4,0.6,0.8,1]\n",
    "dataset_name = 'cora'\n",
    "link_pred = True #whether link prediction task\n",
    "multi_view = True\n",
    "\n",
    "\n",
    "if 'syn' in dataset_name:\n",
    "    assor = \"h0.10-r1\" #the global homohpiyl of synthetic dataset\n",
    "    dataset = CustomDataset(root=\"./input/datasets/{}\".format(dataset_name), name=assor, setting=\"gcn\", seed=15)\n",
    "    G=nx.DiGraph(dataset.adj)\n",
    "    G_label = dataset.labels\n",
    "    G_attr = pd.DataFrame(dataset.features.toarray())\n",
    "    G_attr['nodes'] = G_attr.index\n",
    "else:\n",
    "    iG,G,G_label,G_attr = read_data(dataset_name)\n",
    "\n",
    "data = process_data(G,G_attr,link_pred,multi_view)\n",
    "data = data.to(device)\n",
    "\n",
    "\n",
    "train,test,train_label,test_label = get_pair_wise(dataset_name,test_size=0.15)\n",
    "out_channels=128\n",
    "lr = 0.001\n",
    "epoch = 200\n",
    "aucs, aps = [], []\n",
    "\n",
    "#training ten times\n",
    "for i in range(5):\n",
    "    model = MVGE2(data.x.shape[1],data.x_neighbor.shape[1],out_channels).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    os.makedirs(\"datasets\", exist_ok=True)\n",
    "\n",
    "    best_auc,best_ap = 0,0\n",
    "    #start training\n",
    "    for epoch in range(epoch):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(data.x,data.x_neighbor, data.edge_index, data.edge_index,a,b)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #start evaluating\n",
    "    z,z1,z2 = model.embed(data.x,data.x_neighbor,data.edge_index)\n",
    "    node_embedding = z.cpu().detach().numpy()\n",
    "    node_embedding = pd.DataFrame(node_embedding)\n",
    "    auc,ap = get_score(node_embedding,train,test,train_label,test_label)\n",
    "    aucs.append(auc)\n",
    "    aps.append(ap)\n",
    "    del model\n",
    "print('End of Tainning of 10 times!')\n",
    "print('AUC ROC :{:.4f}±{:.4f}'.format(np.mean(aucs),np.std(aucs)))\n",
    "print('AP: {:.4f}±{:.4f}'.format(np.mean(aps),np.std(aps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
